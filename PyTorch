import time
import matplotlib.pyplot as plt
import numpy as np
import torch
from torch import nn, optim
from sklearn.datasets import load_diabetes
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

# Load and preprocess the data
data = load_diabetes()
X, y = data.data, data.target
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# Convert data to PyTorch tensors
X_train_tensor = torch.tensor(X_train, dtype=torch.float32)
y_train_tensor = torch.tensor(y_train, dtype=torch.float32).view(-1, 1)
X_test_tensor = torch.tensor(X_test, dtype=torch.float32)
y_test_tensor = torch.tensor(y_test, dtype=torch.float32).view(-1, 1)

# Results dictionary
results = {}

# Define the linear regression model
class LinearRegressionModel(nn.Module):
    def __init__(self, input_dim):
        super(LinearRegressionModel, self).__init__()
        self.linear = nn.Linear(input_dim, 1)

    def forward(self, x):
        return self.linear(x)

# Training function
def train_model(X, y, epochs=100, learning_rate=0.01):
    model = LinearRegressionModel(X.shape[1])
    criterion = nn.MSELoss()
    optimizer = optim.SGD(model.parameters(), lr=learning_rate)

    for epoch in range(epochs):
        optimizer.zero_grad()
        outputs = model(X)
        loss = criterion(outputs, y)
        loss.backward()
        optimizer.step()

    return model

# 1. Scalability with Data Size
def pytorch_scalability_experiment():
    dataset_sizes = [int(len(X_train) * p) for p in [0.2, 0.4, 0.6, 0.8, 1.0]]
    training_times = []

    for size in dataset_sizes:
        X_sample, y_sample = X_train_tensor[:size], y_train_tensor[:size]

        start_time = time.time()
        train_model(X_sample, y_sample)
        training_times.append(time.time() - start_time)

    plt.figure()
    plt.plot([p * 100 for p in [0.2, 0.4, 0.6, 0.8, 1.0]], training_times, label="Training Time")
    plt.xlabel("Dataset Size (%)")
    plt.ylabel("Training Time (seconds)")
    plt.title("Scalability with Data Size - PyTorch")
    plt.legend()
    plt.show()

    results['Scalability'] = training_times

# 2. Fault Tolerance
def pytorch_fault_tolerance_experiment():
    X_sample, y_sample = X_train_tensor[:int(len(X_train) * 0.8)], y_train_tensor[:int(len(y_train) * 0.8)]

    try:
        # Simulate fault
        raise RuntimeError("Simulated fault")
    except RuntimeError:
        start_time = time.time()
        train_model(X_sample, y_sample)
        elapsed_time = time.time() - start_time

        plt.figure()
        plt.bar(["Training with Fault"], [elapsed_time], color='orange', label="Time with Fault")
        plt.ylabel("Training Time (seconds)")
        plt.title("Fault Tolerance - PyTorch")
        plt.legend()
        plt.show()

        results['Fault Tolerance'] = elapsed_time

# 3. Communication Overhead
def pytorch_communication_overhead_experiment():
    # Simulate splitting data across workers
    split_X = torch.split(X_train_tensor, len(X_train_tensor) // 4)
    split_y = torch.split(y_train_tensor, len(y_train_tensor) // 4)

    # Measure synchronous training
    start_time = time.time()
    for i in range(4):
        train_model(split_X[i], split_y[i])
    sync_time = time.time() - start_time

    # Measure asynchronous training (simulated)
    start_time = time.time()
    for i in range(4):
        train_model(split_X[i], split_y[i])
    async_time = time.time() - start_time

    plt.figure()
    plt.bar(["Synchronous", "Asynchronous"], [sync_time, async_time], color=['blue', 'green'])
    plt.ylabel("Training Time (seconds)")
    plt.title("Communication Overhead - PyTorch")
    plt.show()

    results['Communication'] = {'Synchronous': sync_time, 'Asynchronous': async_time}

# 4. Ease of Use and Setup
def pytorch_ease_of_use_experiment():
    start_time = time.time()

    # Simulate setup
    _ = LinearRegressionModel(X_train_tensor.shape[1])
    elapsed_time = time.time() - start_time

    plt.figure()
    plt.bar(["Environment Setup"], [elapsed_time], color='purple', label="Setup Time")
    plt.ylabel("Setup Time (seconds)")
    plt.title("Ease of Setup - PyTorch")
    plt.legend()
    plt.show()

    results['Ease of Setup'] = elapsed_time

# General Graph for PyTorch
def general_pytorch_graph():
    aspects = ['Scalability', 'Fault Tolerance', 'Communication (Sync)', 'Communication (Async)', 'Ease of Setup']
    values = [
        sum(results['Scalability']) if 'Scalability' in results else 0,
        results['Fault Tolerance'] if 'Fault Tolerance' in results else 0,
        results['Communication']['Synchronous'] if 'Communication' in results else 0,
        results['Communication']['Asynchronous'] if 'Communication' in results else 0,
        results['Ease of Setup'] if 'Ease of Setup' in results else 0
    ]

    plt.figure()
    plt.bar(aspects, values, color=['red', 'orange', 'blue', 'green', 'purple'])
    plt.ylabel("Time/Performance (seconds)")
    plt.title("PyTorch - General Framework Overview")
    plt.xticks(rotation=45)
    plt.show()

# Run all experiments
pytorch_scalability_experiment()
pytorch_fault_tolerance_experiment()
pytorch_communication_overhead_experiment()
pytorch_ease_of_use_experiment()
general_pytorch_graph()

# Print results
print("Results Summary:")
for key, value in results.items():
    print(f"{key}: {value}")
